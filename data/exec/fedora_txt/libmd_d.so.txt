C2C`
  A !
e'j2$u
@tfY%<#
	2}G
ST7FD-#Z
v[PN
[JUCVT	G
ZMZ6^
SUCVk
U26>T
KNBt
?qaH
jUb>6
QIQ)
?@`<
P\i9
,6ck
~g_. 
__gmon_start__
_fini
__cxa_finalize
_Jv_RegisterClasses
dd_sendrecv2_rvec
dd_sendrecv_rvec
debug
__fprintf_chk
dd_sendrecv_int
fwrite
save_realloc
gmx_debug_at
fputc
over_alloc_dd
stderr
gmx_fatal
dd_move_f_vsites
dd_clear_f_vsites
dd_move_x_constraints
dd_move_x_vsites
dd_constraints_nlocalatoms
dd_clear_local_constraint_indices
dd_clear_local_vsite_indices
dd_make_local_constraints
atom2constraints_moltype
gmx_mtop_atomnr_to_molblock_ind
dd_make_local_vsites
interaction_function
init_domdec_constraints
save_calloc
init_domdec_vsites
strlen
gmx_step_str
init_mdebin
gmx_mtop_ftype_count
getenv
mk_ebin
get_ebin_space
egrp_nm
__sprintf_chk
save_free
_gmx_error
__strdup
do_enxnms
mde_delta_h_coll_init
open_dhdl
gmx_fio_fopen
xvgr_header
xvgr_subtitle
xvgr_legend
upd_mdebin
add_ebin
ebin_increase_count
mde_delta_h_coll_add_dh
constr_rmsd
upd_mdebin_step
print_ebin_header
print_ebin
print_orires_log
pr_ebin
init_enxframe
add_blocks_enxframe
add_subblocks_enxblock
mde_delta_h_coll_handle_block
do_enx
enx_file_pointer
gmx_fio_check_file_position
mde_delta_h_coll_reset
free_enxframe
diagonalize_orires_tensors
reset_ebin_sums
init_energyhistory
mde_delta_h_coll_update_energyhistory
restore_energyhistory_from_state
mde_delta_h_coll_restore_energyhistory
read_nblist
strstr
fgets2
__isoc99_sscanf
__isoc99_fscanf
_range_check
dump_nblist
enlist_names
nrnb_str
domdec_zones
ddglatnr
fflush
sqrt
dd_get_constraint_range
gmx_write_pdb_box
gmx_mtop_atominfo_global
gmx_fio_fclose
n_flexible_constraints
too_many_constraint_warnings
constrain
_where
constrain_lincs
econstr_names
bshakef
do_edsam
settle_proj
set_pbc
pull_constraint
csettle
fputs
constr_rmsd_data
lincs_rmsd_data
lincs_rmsd
make_at2con
done_blocka
set_constraints
dd_make_local_ed_indices
set_lincs
shake_init
settle_init
init_blocka
gen_sblocks
make_invblocka
constr_r_max
init_constraints
please_cite
init_lincs
gmx_mtop_ilistloop_init
gmx_mtop_ilistloop_next
init_edsam
inter_charge_group_constraints
stdin
_IO_getc
__ctype_toupper_loc
abort
exit
fft5d_plan_3d
gmx_parallel_env_initialized
tMPI_Comm_size
tMPI_Comm_rank
gmx_fft_init_many_1d
tMPI_Thread_mutex_lock
fftw_plan_guru_dft
tMPI_Thread_mutex_unlock
gmx_fft_init_many_1d_real
fftw_plan_guru_dft_c2r
fftw_plan_guru_dft_r2c
fft5d_execute
tMPI_Wtime
fftw_execute
TMPI_DOUBLE
gmx_fft_many_1d_real
gmx_fft_many_1d
tMPI_Alltoall
fft5d_destroy
gmx_many_fft_destroy
fft5d_local_size
fft5d_plan_3d_cart
tMPI_Cart_create
tMPI_Cart_get
tMPI_Cart_sub
__printf_chk
fft5d_compare_data
putchar
move_rvecs
pd_index
gmx_tx_rx_real
gmx_wait
pd_shift
move_reals
pd_bshift
move_rborn
move_gpol
move_cgcm
pd_cgindex
gmx_tx
gmx_rx
gmx_tx_wait
gmx_rx_wait
gmx_mtop_atomnr_to_atom
eQMbasis_names
eQMmethod_names
sort_QMlayers
call_QMroutine
init_QMroutine
update_QMMM_coord
mk_QMrec
mk_MMrec
copy_QMrec
mk_QMMMrec
init_QMMMrec
gmx_mtop_atomloop_all_init
gmx_mtop_atomloop_all_next
gmx_mtop_atomnr_to_ilist
gmx_mtop_ilistloop_all_init
gmx_mtop_ilistloop_all_next
update_QMMMrec
set_pbc_dd
pbc_dx_aiuc
gmx_sumi
fopen64
calculate_QMMM
gmx_qmmm_gaussian_empty
TMPI_BYTE
tMPI_Send
tMPI_Recv
tMPI_Sendrecv
tMPI_Alltoallv
TMPI_INT
gather_f_bsplines
make_bsplines
make_dft_mod
sincos
make_bspline_moduli
gmx_pme_destroy
gmx_parallel_3dfft_destroy
gmx_pme_init
tMPI_Comm_split
gmx_parallel_3dfft_init
tMPI_Type_contiguous
tMPI_Type_commit
gmx_pme_calc_energy
gmx_pme_do
wallcycle_start
wallcycle_stop
gmx_parallel_3dfft_execute
gmx_parallel_3dfft_real_limits
gmx_parallel_3dfft_complex_limits
gmx_pmeonly
gmx_pme_pp_init
init_nrnb
gmx_pme_recv_q_x
gmx_pme_send_force_vir_ener
wcycle_get_reset_counters
wallcycle_reset_all
wcycle_set_reset_counters
do_tpi
gmx_mtop_generate_local_top
atoms2md
update_mdatoms
init_enerdata
runtime_start
print_date_and_time
calc_cgcm
opt2fn
gmx_rng_init
read_first_frame
gmx_rng_uniform_real
rotate_conf
ei_names
do_force
calc_dispcorr
gmx_sumd
read_next_frame
runtime_end
close_trj
write_sto_conf_mtop
xvgropen
strcmp
pbc_dx_d
pull_print_output
get_pullgrp_distance
clear_pull_forces
pull_potential
pull_calc_coms
dd_make_local_pull_groups
init_pull
epullg_names
pd_at_range
epull_names
finish_pull
init_ewald_tab
do_ewald
gmx_qhop_add_param
gmx_qhop_set_donor
gmx_qhop_set_acceptor
gmx_qhop_init
add_xml_int
xmlSetProp
add_xml_double
add_xml_char
add_xml_child
xmlNewChild
add_xml_comment
xmlNewComment
gmx_qhops_read
__xmlDoValidityCheckingDefaultValue
xmlParseFile
xmlFreeDoc
gmxlibfn
gmx_qhops_write
xmlNewDoc
xmlCreateIntSubset
xmlNewDocNode
gmx_qhop_get_donor
gmx_qhop_get_acceptor
gmx_qhop_get_param
xmlSetDocCompressMode
__xmlIndentTreeOutput
xmlSaveFormatFileEnc
comm_box_frac
dd_choose_grid
n_bonded_dx
gmx_bcast
eewg_names
pme_load_estimate
ePBC2npbcdim
inputrec2nboundeddim
set_ddbox
dd_bcast
set_ddbox_cr
do_nonbonded
init_neighbor_list
calc_naaj
ci2xyz
dd_get_ns_ranges
ns_realloc_natoms
init_ns
init_grid
pr_bvec
strtol
ncg_mtop
search_neighbours
max_cutoff2
fill_grid
calc_elemnr
calc_ptrs
grid_last
check_grid
print_grid
mv_grid
get_nsgrid_boundaries
grid_first
natoms_beyond_ns_buffer
gmx_fft_transpose_2d
memcpy
gmx_fft_transpose_2d_nelem
pull_d_pbc_dx
atan2
RF_excl_correction
calc_rffac
eel_names
init_generalized_rf
write_traj
do_pbc_mtop
construct_vsites
global_stat
calc_pres
sum_dhdl
dd_partition_system
dd_store_state
init_em
dd_init_local_top
dd_init_local_state
global_stat_init
init_mdoutf
calc_shifts
split_system
pd_cg_range
set_vsite_top
mk_graph
do_cg
do_per_step
dd_charge_groups_global
ftp2fn
gmx_pme_finish
done_mdoutf
do_lbfgs
do_steep
do_nm
gmx_sparsematrix_increment_value
gmx_sparsematrix_init
gmx_mtxio_write
mk_bin
global_stat_destroy
destroy_bin
reset_bin
add_binr
add_bind
sum_bin
extract_binr
extract_bind
dd_print_missing_interactions
open_trn
open_enx
opt2bSet
open_xtc
close_enx
close_xtc
close_trn
fwrite_trn
gmx_fio_flush
write_xtc
dd_collect_vec
dd_collect_state
write_checkpoint
tMPI_Gather
write_dd_pdb
dd_cutoff_twobody
dd_cutoff_mbody
dd_make_reverse_top
gmx_mtop_bondeds_free_energy
dd_make_local_cgs
dd_make_local_top
dd_sort_local_top
gmx_sort_ilist_fe
init_state
gmx_rng_n
make_charge_group_links
dd_bonded_cg_distance
mk_graph_ilist
mk_mshift
done_graph
gmx_qmmm_gamess_empty
gmx_fft_fftw2_empty
init_gb_nblist
print_nblist
fill_log_table
table_log
gb_pd_send
tMPI_Gatherv
tMPI_Bcast
init_gb_plist
init_gb_still
init_gb
gmx_mtop_global_atoms
calc_gb_rad
genborn_allvsall_calc_still_radii
calc_gb_rad_still_sse2_double
calc_gb_rad_hct_obc_sse2_double
genborn_allvsall_calc_still_radii_sse2_double
genborn_allvsall_calc_hct_obc_radii_sse2_double
gmx_invsqrt_exptab
gmx_invsqrt_fracttab
dd_atom_spread_real
tanh
genborn_allvsall_calc_hct_obc_radii
dd_atom_sum_real
gb_bonds_tab
calc_gb_selfcorrections
calc_gb_nonpolar
calc_gb_chainrule
calc_gb_forces
genborn_allvsall_calc_chainrule
calc_gb_chainrule_sse2_double
genborn_allvsall_calc_chainrule_sse2_double
make_gb_nblist
make_local_gb
mde_delta_h_handle_block
floor
gmx_fft_1d
fftw_execute_dft
gmx_fft_1d_real
fftw_execute_dft_c2r
fftw_execute_dft_r2c
gmx_fft_2d
gmx_fft_2d_real
gmx_fft_3d
gmx_fft_3d_real
gmx_fft_destroy
fftw_destroy_plan
fftw_free
gmx_fft_init_3d_real
fftw_malloc
fftw_plan_dft_c2r_3d
fftw_plan_dft_r2c_3d
gmx_fft_init_3d
fftw_plan_dft_3d
gmx_fft_init_2d_real
fftw_plan_dft_c2r_2d
fftw_plan_dft_r2c_2d
gmx_fft_init_2d
fftw_plan_dft_2d
fftw_plan_many_dft_r2c
fftw_plan_many_dft_c2r
gmx_fft_init_1d_real
fftw_plan_many_dft
gmx_fft_init_1d
f_calc_vir
tMPI_Group_incl
tMPI_Comm_create
cost_nrnb
tMPI_Cart_coords
tMPI_Cart_rank
dd_scatterv
gmx_qsort
dd_gather
gmx_fatal_collective
dd_scatter
dd_bcastc
dd_natoms_vsite
dd_move_x
dd_move_f
dd_sendrecv_real
dd_gatherv
gmx_pmeonlynode
get_pme_ddnodes
dd_pme_maxshift_x
dd_pme_maxshift_y
dd_cycles_add
dd_force_flop_start
dd_force_flop_stop
setup_dd_grid
tMPI_Comm_group
tMPI_Group_free
make_dd_communicators
tMPI_Allreduce
init_domain_decomposition
srand
edlb_names
gmx_mtop_global_cgs
wallcycle_have_counter
epbc_names
dd_init_bondeds
set_dd_parameters
reset_dd_statistics_counters
print_dd_statistics
forcerec_set_ranges
make_local_shells
gmx_pme_send_q
destroy_enerdata
sum_epot
reset_enerdata
do_force_lowlevel
pr_rvecs
calc_bonds
calc_bonds_lambda
ewald_LRcorrection
print_nrnb
gmx_pppm_do
do_walls
shift_LRcorrection
set_lincs_matrix
pd_get_constraint_range
acos
pd_move_x_constraints
pd_constraints_nlocalatoms
wallcycle_init
wallcycle_destroy
tMPI_Barrier
wallcycle_sum
wallcycle_print
genborn_sse_dummy
gmx_fft_mkl_empty
gmx_rng_gaussian_table
store_rvec
get_stochd_state
gmx_rng_get_state
set_stochd_state
gmx_rng_set_state
init_update
calc_ke_part
init_ekinstate
update_ekinstate
restore_ekinstate_from_state
set_deform_reference_box
update_tcouple
berendsen_tcoupl
rescale_velocities
vrescale_tcoupl
nosehoover_tcoupl
update_pcouple
berendsen_pcoupl
parrinellorahman_pcoupl
update_constraints
unshift_x
update_box
berendsen_pscale
preserve_box_shape
update_coords
update_orires_history
update_disres_history
correct_ekin
gmx_qmmm_mopac_empty
cshake
crattle
vec_shakef
read_xvg
make_tables
save_calloc_aligned
gmx_erf
gmx_erfc
bDebugMode
make_gb_table
ffclose
make_bonded_table
init_mdatoms
init_vcm
ndof_com
ecm_names
calc_vcm_grp
do_stopcm_grp
check_cm_grp
jacobi
get_center
translate_x
stdout
do_flood
communicate_group_positions
rotate_x
ed_open
dd_make_local_group_indices
do_pbc_first_mtop
unshift_self
construct_vsites_mtop
spread_vsite_f
init_vsite
setup_parallel_vsites
gmx_pppm_init
genborn_allvsall_sse2_single_dummy
set_state_entries
init_gtc_state
init_parallel
bcast_ir_mtop
gmx_gettime
gettimeofday
print_time
gmx_ctime_r
clock
runtime_upd_proc
posres
gmx_pme_send_x
update_forcerec
calc_mu
gmx_pme_receive_f
put_charge_groups_in_box
pd_move_f
do_constrain_first
calc_enervirdiff
ceil
evdw_names
do_pbc_first
p_graph
finish_run
tMPI_Reduce
pr_load
print_flop
print_perf
init_md
update_annealing_target_temp
gmx_qhop_get_value
gmx_strcasecmp
strtod
gmx_qhop_done
make_wall_tables
ftp2ext
init_orca
remove
write_orca_input
feof
fgets
rename
read_orca_output
do_orca
call_orca
init_shell_flexcon
ptype_str
relax_shell_flexcon
pr_rvec
mk_forcerec
set_avcsixtwelve
can_use_allvsall
init_forcerec
check_box
bool_names
calc_ewaldcoeff
gmx_setup_kernels
esol_names
set_shift_consts
pr_forcerec
tMPI_Isend
tMPI_Waitall
tMPI_Irecv
tMPI_Scatter
tMPI_Scatterv
read_mu
fread
tMPI_Wait
gmx_tx_rx_void
split_top
partdec_init_local_state
init_ekindata
accumulate_u
update_ekindata
sum_ekin
calc_temp
symmetrize_ghat
mk_ghat
gknew
wr_ghat
pr_scalar_gk
rd_ghat
gmx_fft_fftpack_empty
get_sum_of_positions
get_center_comm
epcoupltype_names
init_bufstate
destroy_bufstate
trotter_update
init_npt_vars
NPT_energy
gmx_rng_gaussian_real
vrescale_energy
gmx_set_stop_condition
gmx_get_stop_condition
get_nsgrid_boundaries_vac
done_grid
xyz2ci_
range_warn
libgmx_d.so.6
libdl.so.2
libm.so.6
libfftw3.so.3
libxml2.so.2
libpthread.so.0
libc.so.6
__stack_chk_fail
_edata
__bss_start
libmd_d.so.6
GLIBC_2.1
GLIBC_2.0
LIBXML2_2.4.30
GLIBC_2.1.3
GLIBC_2.4
GLIBC_2.3
GLIBC_2.3.4
GLIBC_2.7
[^_]
\;Bl
[^_]
[^_]
[^_]
VUUU
[^_]
[^_]
VUUUi78
VUUU
d$L[^_]
d$<[^_]
d$l[^_]
9V\t*
[^_]
%uT1
dH/d
\lam
[^_]
[^_]
d$p[^]
[^_]
|1~g
d$<[^_]
H(~n
d$L[^_]
[^_]
[^_]
VUUUVS
[^_]
\$8)
D$$ 
[^_]
\$8i
\$()
D$$ 
d$$[]
D$P1
[^_]
D$P1
D$P1
D$$1
D$$1
VUUU
VUUU
d$|[^_]
[^_]
[^_]
[^_]
VUUU
>99}
VUUU
VUUU
d$<[^_]
9p0~
P<t4
[^_]
[^_]
d$\[^_]
[^_]
T$(1
T$$1
[^_]
[^_]
[^_]
[^_]
tK9u
d$l[^_]
VUUU
d$|[^_]
d$L[^_]
d$\[^_]
d$$[]
d$$[]
[^_]
d$$[]
[^_]
t	9M
[^_]
[^_]
gfff
VUUU
[^_]
[^_]
d$t[^_]
d$<[^_]
d$<[^_]
d$L[^_]
d$l[^_]
d$l[^_]
F\+~`
[^_]
d$l[^_]
t%9E
d$L[^_]
[^_]
d$\[^_]
d$|[^_]
d$|[^_]
[^_]
[^_]
d$|[^_]
}4kF4
[^_]
[^_]
gfff
<	v@
[^_]
f. V
d$$[]
d$L[^_]
d$L[^_]
d$L[^_]
[^_]
~k9U
d$|[^_]
D09u
d$l[^_]
[^_]
[^_]
d$<[^_]
d$\[^_]
d$\[^_]
[^_]
[^_]
[^_]
d$ ^_]
t	9E
[^_]
[^_]
MH;Q
d$|[^_]
d$4[]
d$|[^_]
d$,[^_]
z*u(
u79~
[^_]
[^_]
[^_]
[^_]
UUUU
UUUU
[^_]
}$uJ
N(9N
} u9
[^_]
[^_]
d$,[^_]
[^_]
tq~/
D$<	
[^_]
;G$~
d$|[^_]
;F$~
d$|[^_]
;F$~
;G$~
;F$~
[^_]
;F$~
;G$~
;F$~
;G$~
;F$~
;G$~
;F$~
;F$~
;F$~
;G$~
VUUU
d$l[^_]
@P= 
[^_]
d$\[^_]
}?9}
z+u1
B@t!
[^_]
[^_]
[^_]
d$|[^_]
d$|[^_]
d$|[^_]
[^_]
[^_]
[^_]
[^_]
d$L[^_]
d$\1
[^_]
d$l1
[^_]
[^_]
[^_]
[^_]
d$l[^_]
[^_]
d$l[^_]
[^_]
L$0f
[^_]
D$@@
[^_]
[^_]
[^_]
[^_]
[^_]
[^_]
D0 f
D0 f
D0(f
[^_]
EH%@
[^_]
EH%@
[^_]
[^_]
[^_]
d$P[^_]
[^_]
d$L[^_]
[^_]
d$<[^_]
d$<[^_]
P(9P
H(9H
G(9G
[^_]
;J8|
p@t?
d$|[^_]
F(9F
[^_]
d$ [^]
F(9F
N(9N
F(9F
d$L[^_]
[^_]
;J8}`
[^_]
}ekE
d$<[^_]
d$<[^_]
d$|[^_]
d$,1
[^_]
d$<1
[^_]
d$L[^_]
d$\[^_]
[^_]
z*u0
z-u3
[^_]
[^_]
[^_]
[^_]
d$\[^_]
d$l[^_]
[^_]
d$|[^_]
d$|[^_]
[^_]
d$|[^_]
d$|[^_]
d$|[^_]
d$|[^_]
d$|[^_]
d$|[^_]
[^_]
d$L[^_]
d$,[^_]
d$L[^_]
d$\[^_]
[^_]
d$L[^_]
[^_]
[^_]
[^_]
d$L[^_]
[^_]
[^_]
[^_]
d$L[^_]
[^_]
[^_]
[^_]
d$L[^_]
[^_]
[^_]
[^_]
d$\[^_]
[^_]
[^_]
[^_]
[^_]
[^_]
[^_]
[^_]
gfff
d$<[^_]
[^_]
[^_]
Y8[]
Y8[]
d$,[^_]
[^_]
d$l[^_]
d$L[^_]
[^_]
[^_]
[^_]
[^_]
d$\[^_]
(A f
(A 9
[^_]
d$l[^_]
d$l[^_]
~P9N
d$l[^_]
d$<[^_]
P(9P
[^_]
[^_]
d$P^_]
D0 f
P(9P
d$l[^_]
d$|[^_]
d$|[^_]
[^_]
?99|VtD
~v9u
d$|[^_]
B(9B
J(9J
[^_]
B(9B
[^_]
A(9A
P(9P
gfff
P(9P
q(9q
[^_]
[^_]
J(9J
[^_]
[^_]
[^_]
d$,[^_]
d$\[^_]
[^_]
[^_]
9AX~
[^_]
[^_]
G(9G
~(9~
[^_]
q(9q
J(9J
[^_]
Q(9Q
Q(9Q
D$$ 
[^_]
d$|[^_]
J(9J
[^_]
91t.
[^_]
J(9J
[^_]
J(9J
[^_]
<nt><yf
t6f1
<at/
T$ )
[^_]
D$81
d$L[^_]
d$L[^_]
[^_]
d$L[^_]
d$L[^_]
t	9E
G(9G
[^_]
B(9B
w(9w
O(9O
W(9W
;wX|
d$l[^_]
d$<[^_]
d$<[^_]
d$,[^_]
[^_]
[^_]
[^_]
d$0[^_]
[^_]
[^_]
VUUU
VUUU
[^_]
VUUU
VUUU
z"u0
[^_]
[^_]
z*u(
uGiF
[^_]
    
    f
[^_]
    
    f
    
    f
    
    f
    
    f
[^_]
[^_]
[^_]
[^_]
d$d1
[^_]
[^_]
d$L[^_]
[^_]
[^_]
d$,[^_]
d$\[^_]
d$L[^_]
_0t)
JTt"
d$|[^_]
d$|[^_]
[^_]
[^_]
^0;P
D$\1
[^_]
[^_]
z[ua
[^_]
[^_]
[^_]
d$l[^_]
[^_]
d$$[]
d$\[^_]
d$L[^_]
E@9B
[^_]
VUUUS
VUUU
\$D)
[^_]
[^_]
[^_]
[^_]
[^_]
[^_]
d$l[^_]
zTuZ
P f9P"t
z4u6
[^_]
[^_]
zEuC
[^_]
[^_]
[^_]
[^_]
d$<[^_]
O$;U
[^_]
^ptu
[^_]
[^_]
[^_]
d$<[^_]
d$<[^_]
d$L[^_]
[^_]
[^_]
d$l[^_]
`(;U
B09E
`(;U
d$L[^_]
H09M
b(;E
b(;E
d$$[^_]
[^_]
[^_]
[^_]
d$,[^_]
[^_]
NT;VL
;~L}U
;~L|
@09E
D0 f
[^_]
[^_]
[^_]
[^_]
[^_]
[^_]
d$l[^_]
[^_]
d$|[^_]
uU9U
d$<[^_]
d$<[^_]
d$l[^_]
d$L[^_]
[^_]
[^_]
D0 f
D$41
[^_]
\$`1
tJ9u
[^_]
T$h1
[^_]
[^_]
d$,[^_]
[^_]
d$<[^_]
d$$[]
d$,[^_]
d$,[^_]
d$,1
[^_]
d$,[^_]
[^_]
[^_]
[^_]
[^_]
[^_]
[^_]
d$,[^_]
QMMM
excl
.dat
d$l[^_]
VUUU
VUUU
[^_]
orca
d$\[^_]
E,+E(
u(9u,
[^_]
d$\[^_]
[^_]
d$<[^_]
J$~E1
[^_]
d$|[^_]
lk49l
d$|[^_]
[^_]
[^_]
O xk
[^_]
d$$[]
d$D[]
d$D[]
[^_]
A<i488
1u51
z?u=1
;J8}01
[^_]
d$L[^_]
[^_]
[^_]
[^_]
d$$[]
F(9F
[^_]
[^_]
JTt<
[^_]
)w09
[^_]
)_ f
)G09
|8@f
[^_]
[^_]
Yz f
[^_]
d$$[]
d$D[]
d$$[]
d$D[]
d$P[^]
d$$[]
d$\[^_]
d$\[^_]
[^_]
d$l[^_]
d$L[^_]
d$,[^_]
[^_]
[^_]
d$ ^_]
[^_]
[^_]
[^_]
[^_]
[^_]
[^_]
d$\[^_]
d$|[^_]
oW f
D8 f
d$|[^_]
[^_]
[^_]
[^_]
[^_]
d$l[^_]
[^_]
d$L[^_]
[^_]
[^_]
d$,[^_]
d$\[^_]
[^_]
[^_]
[^_]
[^_]
[^_]
d$|[^_]
d$|[^_]
zIuG
[^_]
[^_]
u_;E
Ih;E
d$|[^_]
d$|[^_]
d$L[^_]
d$4[]
d$0^_]
d$4[]
[^_]
`(9}
[^_]
[^_]
[^_]
[^_]
[^_]
[^_]
[^_]
[^_]
d$|[^_]
d$\[^_]
[^_]
[^_]
d$l[^_]
d$L[^_]
d$,[^_]
d$0^_]
d$0^_]
d$0^_]
um;u
rT9E
spac->ind_req
Communicated the counts
spac->bSendAtom
spas->a
spac->ibuf
dd->gatindex
spac->vbuf
spac->vbuf2
 %s%d
dc->con_gl
dc->con_nlocat
il_local->iatoms
dcc->ind_req
 or lincs-order
constraint
dd->constraints
dc->molb_con_offset
dc->molb_ncon_mol
dc->gc_req
dc->ga2la
dd->constraint_comm
Begin init_domdec_vsites
dd->ga2la_vsite
dd->vsite_comm
 or use the -rcon option of mdrun
Begin setup_specat_communication for %s
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/domdec_con.c
dim=%d, dir=%d, searching for %d atoms
Send to node %d, %d (%d) indices, receive from node %d, %d (%d) indices
Requested %d, received %d (tot recv %d)
DD cell %d %d %d: Neighboring cells do not have atoms:
DD cell %d %d %d could only obtain %d of the %d atoms that are connected via %ss from the neighboring cells. This probably means your %s lengths are too long compared to the domain decomposition cell size. Decrease the number of domain decomposition grid cells%s%s.
Done setup_specat_communication
Constraints: home %3d border %3d atoms: %3d
Begin init_domdec_constraints
E	<======  
  ==>
	<====  %s  ====>
	<==  
  ======>
GMX_CONSTRAINTVIR
GMX_VIRIAL_TEMPERATURE
nm^3
kg/m^3
kJ/mol
bar nm
nm/ps
m s/kg
md->igrp
gnm[k]
%s:%s-%s
Number of energy terms wrong
incons
md->tmp_r
md->tmp_v
md->grpnms
T-%s
GMX_NOSEHOOVER_CHAINS
vXi-%d-%s
1/ps
Barostat
vXi-%s
Lamb-%s
Ux-%s
Uy-%s
Uz-%s
md->dhc
Time
%s (%s)
[\lambda]\S-1\N
dH/d\lambda
%s (%s %s)
\DeltaH
%s, %s
T = %g (K), %s = %g
T = %g (K)
setname
%s %s %g
setname[s]
%.4f
Lambda
Step
A V E R A G E S
Invalid print mode (%d)
   Energies (%s)
   Constraint Virial (%s)
   Force Virial (%s)
   Total Virial (%s)
   Pressure (%s)
   Total Dipole (%s)
md->print_grpnms
Epot (%s)
%15s   
%12s   
%15s
Group
%15s   %12s   %12s   %12s
enerhist->ener_ave
enerhist->ener_sum
enerhist->ener_sum_sim
Constr. rmsd
Constr.2 rmsd
Box-XX
Box-YX
Box-YY
Box-ZX
Box-ZY
Box-ZZ
Box-X
Box-Y
Box-Z
Volume
Density
Enthalpy
Box-Vel-XX
Box-Vel-YY
Box-Vel-ZZ
Box-Vel-YX
Box-Vel-ZX
Box-Vel-ZY
1/Viscosity
2CosZ*Vel-X
Mu-X
Mu-Y
Mu-Z
#Surf*SurfTen
Pres-XX
Pres-XY
Pres-XZ
Pres-YX
Pres-YY
Pres-YZ
Pres-ZX
Pres-ZY
Pres-ZZ
ForceVir-XX
ForceVir-XY
ForceVir-XZ
ForceVir-YX
ForceVir-YY
ForceVir-YZ
ForceVir-ZX
ForceVir-ZY
ForceVir-ZZ
ShakeVir-XX
ShakeVir-XY
ShakeVir-XZ
ShakeVir-YX
ShakeVir-YY
ShakeVir-YZ
ShakeVir-ZX
ShakeVir-ZY
ShakeVir-ZZ
	Statistics over %s steps using %s frames
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/mdebin.c
   %12s   %12s   %12s
   %12s   %12.5f   %12.5f
R M S - F L U C T U A T I O N S
Current ref_t for group %s: %8.1f
Mismatch between number of energies in run input (%d) and checkpoint file (%d).
q=Neighborlist:
nri:
%*s%d%*s%d%*s%d%*s%d
iatom
Can not read j
nri = %d  nrj = %d
il_name: %s  Solvent opt: %s
nri: %d  npair: %d
DD zone %d:
 %d %d
  i: %5d  j: %5d
EOF when looking for '%s' in logfile
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/wnblist.c
Can not read nri or nrj (nargs=%d)
Can not read iatom, shift gid or nj (nargs=%d)
mat[%d][%d] changing from %d to %d
i: %d shift: %d gid: %d nj: %d
Found longer constraint distance: r0 %5.3f r1 %5.3f rmax %5.3f
%-6s%5u  %-4.4s%3.3s %c%4d%c   %8.3f%8.3f%8.3f
adjust the lincs warning threshold in your mdp file
Too many %s warnings (%d)
If you know what you are doing you can %sset the environment variable GMX_MAXCONSTRWARN to -1,
but normally it is better to fix the problem
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/constr.c
constrain called for forces displacements while not doing energy minimization, can not do this while the LINCS and SETTLE constraint connection matrices are mass weighted
Constraint error in algorithm %s at step %s
Internal error, SHAKE called for constraining something else than coordinates
step %lld: Water molecule starting at atom %d can not be settled.
Check for bad contacts and/or reduce the timestep if appropriate.
Unknown constraint quantity for settle
Unsupported constraint quantity for virial
coordinates after constraining
Wrote pdb files with previous and current coordinates
ncons: %d, bstart: %d, nblocks: %d
i: %5d, iatom: (%5d %5d %5d), blocknr: %5d
j: %d, nblocks: %d, ncons: %d
DEATH HORROR: sblocks does not match idef->il[F_CONSTR]
Maximum distance for %d constraints, at 120 deg. angles, all-trans: %.3f nm
There are %d flexible constraints
WARNING: step size for flexible constraining = 0
         All flexible constraints will be rigid.
         Will try to keep all flexible constraints at their original length,
         but the lengths may exhibit some drift.
SHAKE is not supported with domain decomposition and constraint that cross charge group boundaries, use LINCS
For this system also velocities and/or forces need to be constrained, this can not be done with SHAKE, you should select LINCS
More than one settle type.
Suggestion: change the least use settle constraints into 3 normal constraints.
Setting the maximum number of constraint warnings to %d
maxwarn < 0, will not stop on constraint errors
 %d %5.3f
 %d %5.3f
TITLE     %s
ATOM
LINCS
SETTLE
vars
vars->vscale_nhc
GMX_SUPPRESS_DUMP
step%sb
initial coordinates
step%sc
at2con.index
at2con.a
path
constr->sblock
Before sorting
Going to sort constraints
After sorting
bstart: %d
i: %5d  sb[i].blocknr: %5u
sblock[%3d]=%5d
inv_sblock
constr->lagr
constr
constr->at2con_mt
Hess2002
Ryckaert77a
Barth95a
Miyamoto92a
GMX_MAXCONSTRWARN
at2cg
[@/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/fft5d.c
Assertion failed for "%s" in file %s, line %d
dump core ? (y/n):
FFT5D: Using %dx%d processor grid, rank %d,%d
FFT5D: N: %d, M: %d, K: %d, P: %dx%d, real2complex: %d, backward: %d, order yz: %d, debug %d
FFT5D: FATAL: Datasize cannot be zero in any dimension
Failed to allocated %u bytes of aligned memory.
FFT5D: Plan s %d rC %d M %d pK %d C %d lsize %d
FFT5D: WARNING: Number of processors %d not evenly dividable by %d
result incorrect on %d,%d at %d,%d,%d: FFT5D:%f reference:%f
%d %d: 
%d %d: copy in lin
%d %d: FFT %d
%d %d: tranposed %d
Compare2
%f %f, 
</builddir/build/BUILD/gromacs-4.5.1/src/mdlib/mvxvf.c
cgcm[0][XX] %f
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/qmmm.c
Semi-empirical QM only supported with Mopac.
Ab-initio Surface-hopping only supported with Gaussian.
Ab-initio calculation only supported with Gamess, Gaussian or ORCA.
qm->xQM
qm->indexQM
qm->shiftQM
qm->atomicnumberQM
Layer %d
nr of QM atoms %d
QMlevel: %s/%s
qm->frontatoms
qm->c6
qm->c12
qmcopy->xQM
qmcopy->indexQM
qmcopy->atomicnumberQM
qmcopy->shiftQM
qmcopy->frontatoms
qmcopy->c12
qmcopy->c6
there we go!
qr->qm
qm_arr
qm_i_particles
mm_j_particles
parallelMMarray
mm->indexMM
mm->shiftMM
mm->xMM
mm->MMcharges
mm->c6
mm->c12
QMMMexcl.dat
excluded
%5d %5d
%5d 
forces
forces2
fshift2
tEv=
@/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/pme.c
PME send node %d %d -> %d grid start %d Communicating %d to %d
PME recv node %d %d <- %d grid start %d Communicating %d to %d
For PME atom communication in dimind %d: nslab %d rank %d
Destroying PME data structures.
Creating PME data structures.
For 2D PME decomposition, #PME nodes must be divisible by the number of nodes in the major dimension
pme does not (yet) work with pbc = screw
The pme grid dimensions need to be larger than pme_order (%d) and in parallel larger than 2*pme_ordern for x and/or y
NOTE: The load imbalance in PME FFT and solve is %d%%.
      For optimal PME load balancing
      PME grid_x (%d) and grid_y (%d) should be divisible by #PME_nodes_x (%d)
      and PME grid_y (%d) and grid_z (%d) should be divisible by #PME_nodes_y (%d)
gmx_pme_calc_energy called in parallel
gmx_pme_calc_energy with free energy
PME: nnodes = %d, nodeid = %d
Can not invert matrix, determinant is zero
/builddir/build/BUILD/gromacs-4.5.1/include/vec.h
%d particles communicated to PME node %d are more than 2/3 times the cut-off out of the domain decomposition cell of their charge group in dimension %c.
This usually means that your system is not well equilibrated.
dimind %d PME node %d send to node %d: %d
Node= %6d, pme local particles=%6d
atc->x
atc->q
atc->f
atc->theta[i]
atc->dtheta[i]
atc->fractx
atc->idx
pme->scounts
pme->rcounts
pme->sdispls
pme->rdispls
pme->sidx
pme->redist_buf
ol->s2g0
ol->s2g1
PME slab boundaries:
  %3d %3d
ol->send_id
ol->recv_id
ol->comm_data
atc->node_dest
atc->node_src
atc->count
atc->rcount
atc->buf_index
ddata
bsp_data
(*pmedata)->nnx
(*pmedata)->nny
(*pmedata)->nnz
(*pmedata)->pmegridA
(*pmedata)->fftgridA
(*pmedata)->cfftgridA
(*pmedata)->pmegridB
(*pmedata)->fftgridB
(*pmedata)->cfftgridB
(*pmedata)->work_mhz
(*pmedata)->work_m2
(*pmedata)->work_denom
(*pmedata)->work_tmp1_alloc
(*pmedata)->work_m2inv
*pmedata
pme->bsp_mod[XX]
pme->bsp_mod[YY]
pme->bsp_mod[ZZ]
pme->pmegridA
pme->pmegrid_sendbuf
pme->pmegrid_recvbuf
pme->pmegridB
pme->work_mhx
pme->work_mhy
pme->work_mhz
pme->work_m2
pme->work_denom
pme->work_tmp1_alloc
pme->work_m2inv
atc->pd
Grid = %p
No grid!
pme->bufv
pme->bufr
PME mesh energy: %g
z>333333
!	@3
with
without
 minus one
direct
reweighted
GMX_TPIC_MASSES
mass_cavity
mass[%d] = %f
%lf%n
GMX_TPI_DUMP
TPI cg %d, atoms %d-%d
x_mol
-rerun
sum_UgembU
-tpi
(kJ mol\S-1\N) / (nm\S3\N)
Time (ps)
TPI energies
-kT log(<Ve\S-\betaU\N>/<V>)
f. -kT log<e\S-\betaU\N>
f. <e\S-\betaU\N>
f. <Ue\S-\betaU\N>
f. <U\sVdW %s\Ne\S-\betaU\N>
f. <U\sdisp c\Ne\S-\betaU\N>
f. <U\sCoul %s\Ne\S-\betaU\N>
f. <U\sRF excl\Ne\S-\betaU\N>
leg[i]
Unknown integrator %s
*bin
t%g_step%d.pdb
t: %f step %d ener: %f
mu %10.3e <mu> %10.3e
  <V>  = %12.5e nm^3
  <mu> = %12.5e kJ/mol
-tpid
\betaU - log(V/<V>)
TPI energy distribution
number \betaU > %g: %9.3e
%6.2f %10d %12.5e
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/tpi.c
Found %d masses in GMX_TPIC_MASSES
WARNING: The temperatures of the different temperature coupling groups are not identical
  The temperature for test particle insertion is %.3f K
Started Test Particle Insertion
Can not do TPI for multi-atom molecule with a twin-range cut-off
WARNING: Your TPI molecule is not centered at 0,0,0
Will insert %d atoms %s partial charges
Will insert %d times in each frame of %s
Re-using the neighborlist %d times for insertions of a single atom in a sphere of radius %f does not make sense
Will use the same neighborlist for %d insertions in a sphere of radius %f
Will insert randomly in a sphere of radius %f around the center of the cavity
f. are averages over one frame
f. <U\sCoul recip\Ne\S-\betaU\N>
Number of atoms in trajectory (%d)%s is not equal the number in the run input file (%d) minus the number of atoms to insert (%d)
  time %.3f, step %d: non-finite energy %f, using exp(-bU)=0
TPI %7d %12.5e %12.5f %12.5f %12.5f
%10.3f %12.5e %12.5e %12.5e %12.5e
A/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/ebin.c
%s-%d: Energies out of range: index=%d nener=%d maxener=%d
Invalid print mode %d in pr_ebin
Cannot write to logfile; maybe you are out of quota?
eb->e
eb->e_sim
eb->enm
Invalid index in pr_ebin: %d
Pres
   %12.5e
Position (nm)
Pull COM
Force (kJ/mol/nm)
Pull force
%d %s%c
setname[g]
pg->ind_loc
pg->weight_loc
r_ij
rjnew
rinew
Pull group %d dr %f %f %f
Pull group %d, iteration %d
Pull inpr %e lambda: %e
Pull ref %8.5f %8.5f %8.5f
UNDEFINED
 weighted
Engin2010
GMX_NO_PULLVIR
pg->weight
, weighted mass %9.3f
pull->dyna
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/pull.c
Distance of pull group %d (%f nm) is larger than 0.49 times the box size (%f)
Pull reference distance for group %d is negative (%f)
The pull constraint reference distance for group %d is <= 0 (%f)
Pull ax^2+bx+c=0: a=%e b=%e c=%e lambda=%e
Pull cur %8.5f %8.5f %8.5f j:%8.5f %8.5f %8.5f d: %8.5f
Pull ref %8s %8s %8s   %8s %8s %8s d: %8.5f %8.5f %8.5f
Pull cor %8.5f %8.5f %8.5f j:%8.5f %8.5f %8.5f d: %8.5f
Pull cor %10.7f %10.7f %10.7f
NOT CONVERGED YET: Group %d:d_ref = %f %f %f, current d = %f
Too many iterations for constraint run: %d
Will apply %s COM pulling in geometry '%s'
between a reference group and %d group%s
with an absolute reference on %d group%s
Cosine weighting is used for groupd %d
Found env. var., will not add the virial contribution of the COM pull forces
Pull groups can not have relative weights and cosine weighting at same time
Can only use cosine weighting with pulling in one dimension (use mdp option pull_dim)
The total%s mass of pull group %d is zero
Pull group %d: %5d atoms, mass %9.3f
, cosine weighting will be used
WARNING: In pull group %d some, but not all of the degrees of freedom
         that are subject to pulling are frozen.
         For pulling the whole group will be frozen.
Can not have frozen atoms in a cylinder pull group
Dynamic reference groups are not supported when using absolute reference!
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/ewald.c
Will do ordinary reciprocal space Ewald sum.
No parallel Ewald. Use PME instead.
et->eir
et->eir[n]
et->tab_xy
et->tab_qxyz
Go away! kmax = %d
!)@/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/gmx_qhop_xml.c
Reading XML file %s. Run a syntax checker such as nsgmls.
xbuf[i]
Unknown element name %s
xml->gqh
Setting
Creating element
Creating doc comment element
qhops.dat
qhops
qhops.dtd
Creating XML document
Creating XML DTD
Creating root element
ISO-8859-1
Saving file
qhop
parameter
nc %2d %2d %2d %2d %2d vol pp %6.4f pbcdx %6.4f pme %9.3e tot %9.3e
Can not have separate PME nodes with 2 or less nodes
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/domdec_setup.c
Guess for relative PME load: %.2f
Could not find an appropriate number of separate PME nodes. i.e. >= %5f*#nodes (%d) and <= #nodes/2 (%d) and reasonable performance wise (grid_x=%d, grid_y=%d).
Use the -npme option of mdrun or change the number of processors or the PME grid dimensions, see the manual for details.
Will use %d particle-particle and %d PME only nodes
This is a guess, check the performance at the end of the log file
Will use %d particle-particle and %d PME only nodes
This is a guess, check the performance at the end of the log file
The value for option -dds should be smaller than 1
Scaling the initial minimum size with 1/%g (option -dds) = %g
To account for pressure scaling, scaling the initial minimum size with %g
Optimizing the DD grid for %d cells with a minimum initial size of %.3f nm
Ewald_geometry=%s: assuming inhomogeneous particle distribution in z, will not decompose in z.
The maximum allowed number of cells is:
Average nr of pbc_dx calls per atom %.2f
*fac
*mfac
mdiv
Using %d separate PME nodes
 %c %d
?ffffff
=Setting global DD grid boundaries to %f - %f
Domain decomposition has not been implemented for box vectors that have non-zero components in directions that do not use domain decomposition: ncells = %d %d %d, box vector[%d] = %f %f %f
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/domdec_box.c
box[%d]  %.3f %.3f %.3f
  v[%d]  %.3f %.3f %.3f
skew_fac[%d] = %f
normal[%d]  %.3f %.3f %.3f
?reallocating neigborlist il_code=%d, maxnri=%d
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/ns.c
Initiating neighbourlist type %d for %s interactions,
with %d SR, %d LR atoms.
Increasing %s nblist %s j size to %d
The charge-group - charge-group neighborlist do not support charge groups larger than %d, found a charge group of size %d
%s, %d: Negative number of short range atoms.
Call your Gromacs dealer for assistance.
Using charge-group - charge-group neighbor lists and kernels
The charge-group - charge-group force loops only support systems with all intra-cg interactions excluded and no inter-cg exclusions, this is not the case for this system.
Max #atoms in a charge group: %d > %d
ns5_core: rs2 = %g, rm2 = %g, rl2 = %g (nm^2)
One of the box vectors has become shorter than twice the cut-off length or box_yy-|box_zy| or box_zz has become smaller than the cut-off.
One of the box diagonal elements has become smaller than twice the cut-off length.
Dividing by zero, file %s, line %d
The neighbor search buffer has negative size: %f nm
nl->iinr
nl->iinr_end
nl->gid
nl->shift
nl->jindex
nlist->jjnr
nlist->jjnr_end
nlist->excl
GMX_NBLISTCG
ns->bexcl
ns->bExcludeAlleg
ns->nl_sr
ns->nsr
ns->nlr_ljc
ns->nlr_one
ns->nl_lr_ljc
ns->nl_lr_one
ns->nl_sr[j]
ns->nl_lr_ljc[j]
ns->nl_lr_one[j]
ns->ns_buf
ns->ns_buf[i]
ns->simple_aaj
ns->bHaveVdW
GMX_DUMP_NL
GMX_DUMP_NL = %d
No work array provided to gmx_fft_transpose_2d_nelem().
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/gmx_fft.c
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/pullutil.c
Can not do cosine weighting for trilinic dimensions
Pull group %d wmass %f wwmass %f invtm %f
Pull cylinder group %d:%8.3f%8.3f%8.3f m:%8.3f
pull->rbuf
pull->dbuf
pull->dbuf_cyl
pdyna->ind_loc
pdyna->weight_loc
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/gmx_parallel_3dfft.c
Invalid transform. Plan and execution don't match regarding reel/complex
*pfft_setup
RF exclusion energy: %g
Tironi95a
No degrees of freedom!
Temperature is %f while using Generalized Reaction Field
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/rf_util.c
epsRF = %10g, I   = %10g, volume = %10g, kappa  = %10g
rc    = %10g, krf = %10g, crf    = %10g, epsfac = %10g
epsRF = %g, rc = %g, krf = %g, crf = %g, epsfac = %g
The electrostatics potential has its minimum at r = %g
WARNING: the generalized reaction field constants are determined from topology A only
B?UUUUUU
%s converged to Fmax < %g in %s steps
%s converged to machine precision in %s steps,
but did not reach the requested Fmax < %g.
%s did not converge to Fmax < %g in %s steps.
Maximum force     = %21.14e on atom %d
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/minimize.c
  %-30s V %12.5e  dVdl %12.5e
Can not do energy minimization with %s, use %s
Polak-Ribiere Conjugate Gradients
   Tolerance (Fmax)   = %12.5e
   F-max             = %12.5e on atom %d
   F-Norm            = %12.5e
CGE: EpotA %f EpotB %f EpotC %f gpb %f
CGE: C (%f) is lower than A (%f), moving C to B
CGE: A (%f) is lower than C (%f), moving A to B
CGE: Found a lower energy %f, moving C to B
Step %d, Epot=%12.6e, Fnorm=%9.3e, Fmax=%9.3e (atom %d)
Reached the maximum number of steps before reaching Fmax < %g
Stepsize too small, or no change in energy.
Converged to machine precision,
but not to the requested precision Fmax < %g
writing lowest energy coordinates.
Performed %d energy evaluations in total.
Cannot do parallel L-BFGS Minimization - yet.
Using %d BFGS correction steps.
Step=%5d, Dmax= %6.1e nm, Epot= %12.5e Fmax= %11.5e, atom= %d%c
You might need to increase your constraint accuracy, or turn
off constraints alltogether (set constraints = none in mdp file)
Constraints present with Normal Mode Analysis, this combination is not supported
Non-cutoff electrostatics used, forcing full Hessian format.
Small system size (N=%d), using full Hessian format.
Using compressed symmetric sparse Hessian format.
Allocating Hessian memory...
starting normal mode calculation '%s'
%d steps.
Maximum force probably not small enough to
 ensure that you are in an 
energy well. 
Be aware that negative eigenvalues may occur
 when the
resulting matrix is diagonalized.
Potential Energy  = %21.14e
Norm of force     = %21.14e
Started %s
state mismatch in do_em_step
s2->x
ems2->f
s2->cg_p
s2->cg_gl
Constraints
Initiating %s
*f_global
ems->s.x
ems->f
*enerd
   Number of steps    = %12d
Doing reorder_partsum
frozen
lastx
lastf
alpha
dx[i]
dg[i]
Low-Memory BFGS Minimizer
ems.s.x
ems.f
Steepest Descents
Normal Mode Analysis
fneg
dfdx
full_matrix
Maximum force:%12.5e
Finished step %d out of %d
Writing Hessian...
MbP?:
H|>:
>/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/stat.c
Cannot write trajectory; maybe you are out of quota?
XTC error - maybe you are out of quota?
gs->itc0
gs->itc1
Summing %d energies
-cpo
-dhdl
-field
E (V/nm)
Applied electric field
*x_xtc
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/domdec_top.c
Constructing atom %d of vsite atom %d is a vsite and non-home
Not enough position restraint coordinates
Not all bonded interactions have been properly assigned to the domain decomposition cells
A list of missing interactions:
Some interactions seem to be assigned multiple times
the first %d missing interactions, except for exclusions:
One or more interactions were multiple assigned in the domain decompostion
%d of the %d bonded interactions could not be calculated because some atoms involved moved further apart than the multi-body cut-off distance (%g nm) or the two-body cut-off distance (%g nm), see option -rdd, for pairs and tabulated bonds also see option -ddcheck
Linking all bonded interactions to atoms
The total size of the atom to interaction index is %d integers
NOTE: The tpr file used for this simulation is in an old format, for less memory usage and possibly more performance create a new tpr file with an up to date version of grompp
There are %d inter charge-group exclusions,
will use an extra communication step for exclusion forces for %s
There are %d inter charge-group virtual sites,
will an extra communication step for selected coordinates and forces
Two-body bonded cut-off distance is %g
dim %d cellmin %f bonded rcheck[%d] = %d, bRCheck2B = %d
We have %d exclusions, check count %d
molecule type '%s' %d cgs has %d cg links through bonded interac.
Of the %d charge groups %d are linked via bonded interactions
Initial maximum inter charge-group distances:
    two-body bonded interactions: %5.3f nm, %s, atoms %d %d
  multi-body bonded interactions: %5.3f nm, %s, atoms %d %d
ril_mt->index
ril_mt->il
link->a
il->iatoms
vsite_pbc[ftype-F_VSITE2]
idef->iparams_posres
%20s of %6d missing %6d
exclusions
assigned
Molecule type '%s'
%20s atoms
     
 global
dd_dump_err
nint_mt
rt->ril_mt
rt->mbi
Making local topology
dd->la2lc
lexcls->index
lexcls->a
state_global->ld_rng
state_local->ld_rng
state_global->ld_rngi
state_local->ld_rngi
link
link->index
ril->index
ril->il
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/genborn.c
genborn.c: print_nblist, natoms=%d
Bad gb algorithm for all-vs-all interactions
Unknown double precision sse-enabled algorithm for Born radii calculation: %d
Increasing GB neighbourlist j size to %d
lists->list
ai == aj
list->aj
ai=%d, aj=%d
sendc
disp
vsol
born->gpol_still_work
born
born->drobc
born->bRad
born->param_globalindex
born->gpol_globalindex
born->vsolv_globalindex
born->gb_radius_globalindex
born->use_globalindex
fr->invsqrta
fr->dvda
born->gpol_hct_work
born->log_table
born->work
born->count
born->nblist_work
fr->dadx_rawptr
Unknown GB algorithm
fr->gblist.jjnr
i == list->aj[k]
born->gpol
born->vsolv
born->gb_radius
born->param
born->use
?es-8R
?es-8R
+Wd<@
>/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/mdebin_bar.c
energy history number of delta_h histograms != inputrec's number
No delta_h histograms in energy history
dhc->dh
dh->dh
dh->bin[i]
delta_h array not big enough!
enerhist->dht
enerhist->dht->ndh
enerhist->dht->dh
<FFT plan mismatch - bad plan or direction.
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/gmx_fft_fftw3.c
Invalid opaque FFT datatype pointer.
Error initializing FFTW3 plan.
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/domdec.c
Unknown state entry encountered in rotate_state_atom
dd->ncg_zone is not up to date
/builddir/build/BUILD/gromacs-4.5.1/include/gmx_ga2la.h
PME slab communication range for dim %d is %d
Reallocating state: currently %d, required %d, allocating %d
Using static load balancing for the %s direction
Incorrect or not enough DD cell size entries for direction %s: '%s'
zone d0 %d d1 %d d2 %d  min0 %6.3f max1 %6.3f mch0 %6.3f mch1 %6.3f p1_0 %6.3f p1_1 %6.3f
Cell fraction d %d, max0 %f, min1 %f
DD node %d: global atom %d occurs twice: index %d and %d
DD node %d: global atom %d marked as local atom %d, which is larger than nat_tot (%d)
DD node %d: global atom %d marked as local atom %d, which has global atom index %d
DD node %d, %s: %d global atom indices, %d local atoms
DD node %d, %s: local atom %d, global %d has no global index
DD node %d, %s: cg %d, global cg %d is not marked in bLocalCG (ncg_home %d)
DD node %d, %s: In bLocalCG %d cgs are marked as local, whereas there are %d
DD node %d, %s: %d atom/cg index inconsistencies
 Average load imbalance: %.1f %%
 Part of the total run time spent waiting due to load imbalance: %.1f %%
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds:
 Average PME mesh/force load: %5.3f
 Part of the total run time spent waiting due to PP/PME imbalance: %.1f %%
NOTE: %.1f %% performance was lost due to load imbalance
      in the domain decomposition.
      You might want to use dynamic load balancing (option -dlb.)
      You might want to decrease the cell size limit (options -rdd, -rcon and/or -dds).
NOTE: %.1f %% performance was lost because the PME nodes
      had %s work to do than the PP nodes.
      You might want to %s the number of PME nodes
      or %s the cut-off and the grid spacing.
Step %s: the dynamic load balancing could not balance dimension %c: box size %f, triclinic skew factor %f, #cells %d, minimum cell size %f
Internal error a (%d) != nat (%d)
Reallocating forcerec: currently %d, required %d, allocating %d
Finished setting up DD communication, zones:
ordered sort cgs: stationary %d moved %d
The box size in direction %c (%f) times the triclinic skew factor (%f) is too small for a cut-off of %f with %d domain decomposition cells, use 1 or more than %d %s or increase the box size in this direction
With screw pbc the unit cell can not have non-zero off-diagonal x-components
pbc=screw with non-zero box_zy is not supported
Charge group distribution at step %s:
The maximum number of communication pulses is:
The minimum size for domain decomposition cells is %.3f nm
The requested allowed shrink of DD cells (option -dds) is: %.2f
The allowed shrink of domain decomposition cells is:
The initial number of communication pulses is:
The initial domain decomposition cell size is:
The maximum allowed distance for charge groups involved in interactions is:
(the following are initial values, they could change due to box deformation)
multi-body bonded interactions
atoms separated by up to %d constraints
glatnr called with %d, which is larger than the local number of atoms (%d)
The charge group starting at atom %d moved than the distance allowed by the domain decomposition (%f) in direction %c
The charge group starting at atom %d moved than the distance allowed by the domain decomposition in direction %c
Old coordinates: %8.3f %8.3f %8.3f
New coordinates: %8.3f %8.3f %8.3f
Old cell boundaries in direction %c: %8.3f %8.3f
New cell boundaries in direction %c: %8.3f %8.3f
A charge group moved too far between two domain decomposition steps
This usually means that your system is not well equilibrated
Sending ddim %d dir %d: ncg %d nat %d
The state does not the domain decomposition state
DD icg %d out of range: izone (%d) >= nizone (%d)
Attempted to collect a vector for a state for which the charge group distribution is unknown
Initial charge group distribution: 
%-6s%5u %-4.4s %3.3s %c%4d%c   %8.3f%8.3f%8.3f
Receive coordinates from PP nodes:
DD rank %d neighbor ranks in dir %d are + %d - %d
Making %dD domain decomposition %d x %d x %d
Making %dD domain decomposition grid %d x %d x %d, home cell index %d %d %d
Can only do 1, 2 or 3D domain decomposition
Internal inconsistency in the dd grid setup
Finished making load communicators
#pmenodes (%d) is not a multiple of nx*ny (%d*%d) or nx*nz (%d*%d)
Will not use a Cartesian communicator for PP <-> PME
Will use a Cartesian communicator for PP <-> PME: %d x %d x %d
MPI rank 0 was renumbered by MPI_Cart_create, we do not allow this
Cartesian nodeid %d, coordinates %d %d %d
Order of the nodes: PP first, PME last
Interleaving PP and PME nodes
Will use a Cartesian communicator: %d x %d x %d
Domain decomposition nodeid %d, coordinates %d %d %d
My pme_nodeid %d receive ener %d
Initializing Domain Decomposition on %d nodes
Found env.var. %s = %s, using value %d
Will use two sequential MPI_Sendrecv calls instead of two simultaneous non-blocking MPI_Irecv and MPI_Isend pairs for constraint and vsite communication
Will load balance based on FLOP count
NOTE: dynamic load balancing is only supported with dynamics, not with integrator '%s'
NOTE: Cycle counting is not supported on this architecture, will not use dynamic load balancing
NOTE: reproducability requested, will not use dynamic load balancing
WARNING: reproducability requested with dynamic load balancing, the simulation will NOT be binary reproducable
Death horror: undefined case (%d) for load balancing choice
Will sort the charge groups at every domain (re)decomposition
Will sort the charge groups every %d steps
Will not sort the charge groups
NOTE: Periodic molecules: can not easily determine the required minimum bonded cut-off, using half the non-bonded cut-off
Minimum cell size due to bonded interactions: %.3f nm
Estimated maximum distance required for P-LINCS: %.3f nm
This distance will limit the DD cell size, you can override this with -rcon
User supplied maximum distance required for P-LINCS: %.3f nm
Using domain decomposition order z, y, x
ERROR: The initial cell size (%f) is smaller than the cell size limit (%f)
The initial cell size (%f) is smaller than the cell size limit (%f), change options -dd, -rdd or -rcon, see the log file for details
Change the number of nodes or mdrun option %s%s%s
There is no domain decomposition for %d nodes that is compatible with the given box and a minimum cell size of %g nm
Look in the log file for details on the domain decomposition
Domain decomposition grid %d x %d x %d, separate PME nodes %d
The size of the domain decomposition grid (%d) does not match the number of nodes (%d). The total number of nodes is %d
The number of separate PME node (%d) is larger than the number of PP nodes (%d), this is not supported.
PME domain decomposition: %d x %d x %d
Bonded atom communication beyond the cut-off: %d
cellsize limit %f
With pbc=%s can only do domain decomposition in the x-direction
Domain decomposition does not support simple neighbor searching, use grid searching or use particle decomposition
Domain decomposition does not work with nstlist=0
comm-mode angular will give incorrect results when the comm group partially crosses a periodic boundary
Can not have separate PME nodes without PME electrostatics
When dynamic load balancing gets turned on, these settings will change to:
Volume fraction for all DD zones: %f
    D O M A I N   D E C O M P O S I T I O N   S T A T I S T I C S
 av. #atoms communicated per step for force:  %d x %.1f
 av. #atoms communicated per step for vsites: %d x %.1f
 av. #atoms communicated per step for LINCS:  %d x %.1f
get_load_distribution finished
DD  load balancing is limited by minimum cell size in dimension
At step %s the performance loss due to force load imbalance is %.1f %%
NOTE: the minimum cell size is smaller than 1.05 times the cell size limit, will not turn on dynamic load balancing
NOTE: Turning on dynamic load balancing
Internal inconsistency state_local->ddp_count (%d) > dd->ddp_count (%d)
Internal inconsistency state_local->ddp_count_cg_gl (%d) != state_local->ddp_count (%d)
The %c-size of the box (%f) times the triclinic skew factor (%f) is smaller than the number of DD cells (%d) times the smallest allowed cell size (%f)
Inconsistent DD boundary staggering limits!
dim %d boundary %d %.3f < %.3f < %.3f < %.3f < %.3f
Relative bounds dim %d  cell %d: %f %f
WARNING step %s: direction %c, cell %d too small: %f
(Re)allocing cd for %c to %d pulses
Increasing the number of cell to communicate in dimension %c to %d for the first time
cell_x[%d] %f - %f skew_fac %f
Step %s: The %c-size (%f) times the triclinic skew factor (%f) is smaller than the smallest allowed cell size (%f) for domain decomposition grid cell %d %d %d
Step %s: The domain decomposition grid has shifted too much in the %c-direction around cell %d %d %d
Step %s, sorting the %d home charge groups
ga2la->lal
state->x
state->v
state->sd_X
state->cg_p
Relative cell sizes:
rank
dd->comm->root[dim_ind]
root->cell_f
root->old_cell_f
root->bCellMin
root->cell_f_max0
root->cell_f_min1
root->bound_min
root->bound_max
root->buf_ncd
dd->comm->cell_f_row
dd->comm->load[dim_ind].load
have
after partitioning
ddpme->pp_min
ddpme->pp_max
*dim_f
decrease
less
increase
more
Unknown dim %d
 %c %d %%
enforce_limits: %d %d
Setting up DD communication
bBondComm %d, r_bc %f
skew_fac_01 %f
ind->index
comm->buf_int
v->v
comm->buf_int2
fr->cg_cm
fr->cginfo
sort->sort1
sort->sort2
sort->sort_new
qsort cgs: %d new home %d
sort->ibuf
grid_r
dd_grid
%s_%s.pdb
%6.2f%6.2f
CONECT
%6s%5d%5d
cells
processors
tmp_nalloc
tmp_ind
tmp_ind[i]
dd->index_gl
dd->cgindex
Home charge groups:
 %c %.2f
 %c %.2f nm
non-bonded interactions
%40s  %-7s %6.3f nm
(-rdd)
two-body bonded interactions
(-rcon)
virtual site constructions
Step %s:
distance out of cell %f
comm->cggl_flag[mc]
comm->cgcm_state[mc]
Finished repartitioning
state->cg_gl
%s_%s_n%d.pdb
*my_ddnodes
dd->comm->root
Making load communicators
dd->comm->load
dd->comm->mpi_comm_load
particle-particle
PME-mesh
GMX_NO_CART_REORDER
pmenodes
pmenode[%d] = %d
Unknown dd_node_order=%d
This is a %s only node
comm->ddindex2ddnodeid
comm->ddindex2simnodeid
The master rank is %d
ma->ncg
ma->index
ma->cg
ma->nat
ma->ibuf
ma->cell_x
ma->cell_x[i]
ma->vbuf
 or your LINCS settings
 or -dds
-rdd
-rcon
comm->cggl_flag
comm->cgcm_state
GMX_DD_SENDRECV2
GMX_DLB_FLOP
GMX_DD_LOAD
GMX_DD_SORT
GMX_DD_DUMP
GMX_DD_DUMP_GRID
GMX_DD_DEBUG
Unknown dlb_opt
Dynamic load balancing: %s
comm->sort
GMX_DD_ORDER_ZYX
GMX_PMEONEDD
comm->slb_frac
bLocalCG
The DD cut-off is %f
GMX_DD_NPULSE
comm->cd[d].ind
ga2la->laa
get_load_distribution start
DD  step %s
  vol min/aver %5.3f%c
 load imb.: force %4.1f%%
  pme mesh/force %5.3f
vol %4.2f%c 
imb F %2d%% 
pme/F %4.2f 
step %s, imb loss %f
cd->ind
dd_dump
auto
UUUUUU
nsearch = %d
enerd->grpp.ener[i]
enerd->enerpart_lambda
Walls
fshift after SR
Ewald excl./charge/dip. corr.
PME mesh
Ewald long-range
vir_el_recip after corr
fshift after LR Corrections
RF exclusion correction
fshift after bondeds
Creating %d sized group matrix for energies
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/force.c
dvdl: %f, non-linear %f + linear %f
enerdiff lam %g: non-linear %f linear %f*%f
Step %s: non-bonded V and dVdl for node %d:
The bonded interactions are not sorted for free energy
TPI with PME currently only works in a 3D geometry with tin-foil boundary conditions
No such electrostatics method implemented %s
Error %d in long range electrostatics routine %s
Vlr = %g, Vcorr = %g, Vlr_corr = %g
VdW and Coulomb SR particle-p.
A constraint is connected to %d constraints, this is more than the %d allowed for constraints participating in triangles
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/clincs.c
Of the %d constraints %d participate in triangles
There are %d couplings of which %d in triangles
Initializing%s LINear Constraint Solver
The number of constraints is %d
There are inter charge-group constraints,
will communicate selected coordinates each lincs iteration
%d constraints are involved in constraint triangles,
will apply an additional matrix expansion of order %d for couplings
between constraints inside triangles
Building the LINCS connectivity
Number of constraints is %d, couplings %d
   Rel. Constraint Deviation:  RMS         MAX     between atoms
       Before LINCS          %.6f    %.6f %6d %6d
        After LINCS          %.6f    %.6f %6d %6d
Step %s, time %g (ps)  LINCS WARNING
relative constraint deviation after LINCS:
rms %.6f, max %.6f (between atoms %d and %d)
bonds that rotated more than %g degrees:
 atom 1 atom 2  angle  previous, current, constraint length
 %6d %6d  %5.1f  %8.4f %8.4f    %8.4f
 Parallel
Hess2008a
Hess97a
li->bllen0
li->ddist
li->bla
li->blc
li->blc1
li->blnr
li->bllen
li->tmpv
li->tmp1
li->tmp2
li->tmp3
li->lambda
li->triangle
li->tri_bits
li->blbnb
li->blmf
li->blmf1
li->tmpncc
?/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/gmx_wallcycle.c
Will call MPI_Barrier before each cycle start/stop call
Will time all the code during the run
GMX_CYCLE_ALL is incompatible with threaded code
     R E A L   C Y C L E   A N D   T I M E   A C C O U N T I N G
 Computing:         Nodes     Number     G-Cycles    Seconds     %c
-----------------------------------------------------------------------
 %-19s %4d %10s %12.3f %10.1f   %5.1f
NOTE: %d %% of the run time was spent communicating energies,
      you might want to use the -gcom option of mdrun
GMX_CYCLE_BARRIER
wc->wcc
GMX_CYCLE_ALL
wc->wcc_all
cyc_all
buf_all
%10d
%-9s
Rest
Total
PP during PME
Domain decomp.
DD comm. load
DD comm. bounds
Vsite constr.
Send X to PME
Comm. coord.
Neighbor search
Born radii
Force
Wait + Comm. F
PME redist. X/F
PME spread/gather
PME 3D-FFT
PME solve
Wait + Comm. X/F
Wait + Recv. PME F
Vsite spread
Write traj.
Update
Comm. energies
Test
_/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/genborn_allvsall.c
aadata->jindex_gb
aadata->exclusion_mask_gb
aadata->exclusion_mask_gb[i]
aadata
]a@3
]a@/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/update.c
SD const tc-grp %d: b %g  c %g  d %g
update_coords called for velocity without VV integrator
Don't know how to update coordinates
dekin = %g, ekin = %g  vcm = (%8.4f %8.4f %8.4f)
sd->sd_V
sd->bd_rf
sd->sdc
sd->sdsig
ekinstate->ekinh
ekinstate->ekinf
ekinstate->ekinh_old
ekinstate->ekinscalef_nhc
ekinstate->ekinscaleh_nhc
ekinstate->vscale_nhc
upd->xp
constraint virial
dekin
mv = (%8.4f %8.4f %8.4f)
?UUUUUU
?UUUUUU
?UUUUUU
Bndx bonded %d exclusions %d
nw %g nqlj %g nq %g nlj %g
cost_bond   %f
cost_pp     %f
cost_spread %f
cost_fft    %f
cost_solve  %f
Estimate for relative PME load: %.3f
L?/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/shakef.c
Shake did not converge in %d steps
Inner product between old and new vector <= 0.0!
constraint #%d atoms %u and %u
    i     mi      j     mj      before       after   should be
%5d  %5.2f  %5d  %5.2f  %10.5f  %10.5f  %10.5f
shaked->rij
shaked->M2
shaked->tt
shaked->dist2
$tIV'''
V'=0
Tabscale = %g points/nm
gamma
WARNING: %s
td->x
td->v
td->f
yy[i]
libfn
1-4 
Modified
Generated
dtab.xvg
rtab.xvg
ctab14.xvg
dtab14.xvg
rtab14.xvg
Invalid eeltype %d
table.tab
Setting up tables
%15.10e  %15.10e  %15.10e
gbctab.xvg
tab.tab
LJ12
LJ6Shift
LJ12Shift
RF-zero
COUL
Ewald
Ewald-Switch
Ewald-User
Ewald-User-Switch
LJ6Switch
LJ12Switch
COULSwitch
LJ6-Encad shift
LJ12-Encad shift
COUL-Encad shift
EXPMIN
USER
Trying to read file %s, but nr columns = %d, should be %d
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/tables.c
The first distance in file %s is %f nm instead of %f nm
The angles in file %s should go from %f to %f instead of %f to %f
Read user tables from %s with %d data points.
In table file '%s' the x values are not equally spaced: %f %f %f
Out of range potential value %g in file '%s'
Out of range force value %g in file '%s'
Force generation for dihedral tables is not (yet) implemented
Generating forces for table %d, boundary conditions: V''' at %g, %s at %g
Can not generate splines with third derivative boundary conditions with less than 4 (%d) points
The left third derivative is %g
The right third derivative is %g
For the %d non-zero entries for table %d in %s the forces deviate on average %d%% from minus the numerical derivative of the potential
NOTE: All elements in table %s are zero
Invalid vdwtype %d in %s line %d
Tables in file %s not long enough for cut-off:
	should be at least %f nm
Table type %d not implemented yet. (%s,%d)
%s table with %d data points for %s%s.
Tabscale = %g points/nm
zt?V
L@/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/mdatom.c
There are %d atoms and %d charges for free energy perturbation
md->massA
md->massB
md->massT
md->invmass
md->chargeA
md->chargeB
md->typeA
md->typeB
md->ptype
md->cTC
md->cENER
md->cACC
md->cFREEZE
md->cVCM
md->cORF
md->bPerturbed
md->cU1
md->cU2
md->bQM
9vcm
vcm->group_j
vcm->group_x
vcm->group_i
vcm->group_w
vcm->group_p
vcm->group_v
vcm->group_mass
vcm->group_name
vcm->group_ndf
%3d:  %s
Inertia tensor
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/vcm.c
Can not have angular comm removal with pbc=%s
Center of mass motion removal mode is %s
We have the following groups for center of mass motion removal:
Can not stop center of mass: maybe 2dimensional system
Can not invert matrix, determinant = %e
Large VCM(group %s): %12.5f, %12.5f, %12.5f, Temp-cm: %12.5e
Group %s with mass %12.5e, Ekrot %12.5e Det(I) = %12.5e
  COM: %12.5f  %12.5f  %12.5f
  P:   %12.5f  %12.5f  %12.5f
  V:   %12.5f  %12.5f  %12.5f
  J:   %12.5f  %12.5f  %12.5f
  w:   %12.5f  %12.5f  %12.5f
Dwo = %g, wh =%g, wohh = %g, rc = %g, ra = %g
rb = %g, rc2 = %g, dHH = %g, dOH = %g
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/csettle.c
settled
yE>/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/edsam.c
Could not find input parameter %s at expected position in edsam input-file (.edi)
line read instead is %s
ED: Will add %15.8e to the reference projection of eigenvector %d at each time step.
Expected 4 (not %d) values for flooding vec: <nr> <spring const> <refproj> <refproj-slope>
Expected 2 values for flooding vec: <nr> <stpsz>
  fixed increment radius = %f
  acceptance radius      = %f
  contracting radius     = %f
  NOTE: none of the ED options mon/linfix/linacc/radfix/radacc/radcon were chosen for dataset #%d!
%d.th FL: %s %12.5e %12.5e %12.5e
ED sampling will be performed!
Please switch on domain decomposition to use essential dynamics in parallel.
ED: Initializing essential dynamics constraints.
edi file %s (dataset #%d) was made for %d atoms, but the simulation contains %d atoms.
wrong magic number: Use newest version of make_edi to produce edi file
Nr of atoms in %s (%d) does not match nr of md atoms (%d)
ED: Note: Reference and average structure are composed of the same atom indices.
No complete ED data set found in edi file %s.
FL_HEADER: Flooding of matrix %d is switched on! The flooding output will have the following format:
ED: Flooding of matrix %d is switched on.
FL_HEADER: Step Efl Vfl deltaF
ED: Initial RMSD from reference after fit = %f nm (dataset #%d)
edi->buf->do_edsam->shifts_xcoll
edi->buf->do_edsam->extra_shifts_xcoll
edi->buf->do_edsam->shifts_xc_ref
edi->buf->do_edsam->extra_shifts_xc_ref
(s->anrs)
(s->x)
s->c_ind
(s->x_old)
(s->m)
(s->sqrtm)
(ev->ieig)
(ev->stpsz)
(ev->xproj)
(ev->fproj)
(ev->refproj)
(ev->vec)
(ev->vec[i])
(ev->refproj0)
(ev->refprojslope)
edi->buf->do_edfit
loc->omega
loc->om
loc->omega[i]
loc->om[i]
IROT=0
edi->buf->fit_to_ref
edi->buf->fit_to_ref->xcopy
NUMBER OF EIGENVECTORS
tvec->ieig
tvec->stpsz
tvec->vec
tvec->xproj
tvec->fproj
tvec->refproj
tvec->refproj0
tvec->refprojslope
%d%lf
tvec->vec[i]
%le%le%le
Initial projections:
Step %s, ED #%d  
  RMSD %f nm
  Efl=%f  deltaF=%f  Vfl=%f
  Monitor eigenvectors
 %d: %12.5e 
  Linfix  eigenvectors
  Linacc  eigenvectors
  Radfix  eigenvectors
  Radacc  eigenvectors
  Radcon  eigenvectors
Ref. projs.: 
FL_FORCES: 
ed->edpar
ED: Reading edi file %s
edi_read
Wrong magic number %d in %s
FITMAS
ANALYSIS_MAS
OUTFRQ
MAXLEN
SLOPECRIT
PRESTEPS
DELTA_F0
INIT_DELTA_F
EFL_NULL
ALPHA2
HARMONIC
NREF
edi->sref.anrs
edi->sref.x
edi->sref.x_old
edi->sav.anrs
edi->sav.x
edi->sav.x_old
edi->star.anrs
edi->star.x
edi->sori.anrs
edi->sori.x
ED: Found %d ED dataset%s.
edi->sref.m
edi->sav.sqrtm
edi->sav.m
edi->buf
x_pbc
xfit
xstart
(ed->edpar)
(edi->next_edi)
edi->sav.c_ind
edi->sref.c_ind
edi->buf->do_edsam
edi->buf->do_edsam->xcoll
edi->buf->do_edsam->xc_ref
edi->flood.forces_cartesian
edi->buf->do_radcon
loc->proj
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/vsite.c
vsite %d match pbc with atom %d
vsite atom %d  cg %d - %d pbc atom %d
No such vsite type %d in %s, line %d
Functiontypes for vsites wrong
pbc_set
vsite_pbc
a2cg
vsite->vsite_pbc_molt
vsite->vsite_pbc_loc_nalloc
vsite->vsite_pbc_loc
vsite->vsitecomm
PPPM is not functional in the current version, we plan to implement PPPM through a small modification of the PME code.
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/pppm.c
PPPM temporarily disabled while working on 2DPME
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/init.c
state->ld_rng
state->ld_rngi
Removing pbc first time
graph
, will finish %s
 performance: %.1f ns/day    
%s on node %d %s
cgcm
LR non-bonded
vir_part
Com pull
vir_force
savex
Dispersion correction
do_pbc_first 1
do_pbc_first 2
Done rmpbc
nrnb_tot
nrnb_all
Berendsen84a
Bussi2007a
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/sim_util.c
, remaining runtime: %5d s          
%10g  %10g  %10g  %10g #FIELD
step %s  atom %6d  x %8.3f %8.3f %8.3f  force %12.5e
vcm: start=%d, homenr=%d, end=%d
Constraining the starting coordinates (step %s)
Constraining the coordinates at t0-dt (step %s)
vcm: %8.3f  %8.3f  %8.3f, total mass = %12.5e
With dispersion correction rvdw-switch can not be zero for vdw-type = %s
WARNING: using dispersion correction with user tables
Dispersion correction is not implemented for vdw-type = %s
Long Range LJ corr.: <C6> %10.4e, <C12> %10.4e
Long Range LJ corr.: Epot %10g, Pres: %10g, Vir: %10g
Long Range LJ corr.: Epot %10g
@/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/gmx_qhop_parm.c
gqh->name
gqh->value
gqh->unit
gqh->name[i]
gqh->value[i]
gqh->unit[i]
gqh->donor
gqh->acceptor
Reading user tables for %d energy groups with %d walls
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/wall.c
An atom is beyond the wall: coordinates %f %f %f, distance %f
You might want to use the mdp option wall_r_linpot
fr->wall_tab
fr->wall_tab[w]
_%s_%s.%s
b0@k{
b0@ *"
s@;?
m0_$@
m0_$@
TUUU
TUUU
?/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/qm_orca.c
#input-file generated by gromacs
No information on the calculation given in <%s>
%3d %10.7lf  %10.7lf  %10.7lf
%8.4lf %10.7lf  %10.7lf  %10.7lf
BASENAME
qm->orca_basename
no $BASENAME
orca initialised...
orcaInput
%s.inp
addInputFilename
%s.ORCAINFO
!QMMMOpt TightSCF
%geom TS_Search EF end
!EnGrad TightSCF
qm_orca.c
call
%geom
   Constraints 
        {C %d C}
     end
   end
exclInName
exclOutName
%s.LJ.Excl
LJCoeffFilename
%s.LJ
%LJCOEFFICIENTS "
%s%s%s
%10.7lf  %10.7lf
*xyz %2d%2d
pcFilename
%s.pc
%pointcharges "
%s.xyz
Unexpected end of ORCA output
%s%lf%lf%lf
%s.engrad
%s.pcgrad
orca
%s %s.inp >> %s.out
Calling '%s'
Call to '%s' failed
QMgrad
MMgrad
D/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/shellfc.c
RELAX: Using prediction for initial shell placement
nsi is %d should be within 0 - %d. aS = %d
Can not handle more than three bonds per shell
polarize can not be used with qA != qB
water_pol can not be used with qA != qB
Something weird with shells. They may not be bonded to something
Will never predict shell positions
Will always initiate shell positions
NOTE: there all shells that are connected to particles outside thier own charge group, will not predict shells positions during the run
MDStep=%5s/%2d EPot: %12.8e, rmsF: %6.2e
SHELL %5d, force %10.5f  %10.5f  %10.5f, |f| %10.5f
step %s: EM did not converge in %d iterations, RMS force %.3f
shfc->adir_xnold
shfc->adir_xnew
Shell %d has %d nuclei!
SHELL DATA
Nucl3
Nucl2
Nucl1
Force k
Shell
%5s  %8s  %5s  %5s  %5s
%5d  %8.3f  %5d
  %5d  %5d
shell_index
There are: %d %ss
shfc
Death Horror: %s, %d
Weird stuff in %s, %d
GMX_NOPREDICT
GMX_FORCEINIT
shfc->x[i]
shfc->f[i]
shfc->acc_dir
shfc->x_old
x b4 do_force
df = %g  %g
force0
, dir. rmsF: %6.2e
%17s: %14.10e
SHELLSTEP %s
shell[%d] = %d
fshell
xold
RELAX: pos[Min]  
RELAX: pos[Try]  
RELAX: force[Min]
RELAX: force[Try]
F na do_force
SHELL ITER %d
SHELL-X
Swapping Min and Try
?333333
?A bonded table number is smaller than 0: %d
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/forcerec.c
No table file name passed, can not read table, can not do non-bonded interactions
WARNING: There are no atom pairs for dispersion correction
Average C6 parameter is: %10g
Average C12 parameter is: %10g
Long Range LJ corr.: <C6> %10.4e
NOTE: Can not use all-vs-all force loops, because there are multiple energy monitor groups; you might get significantly higher performance when using only a single energy monitor group.
Using accelerated all-vs-all kernels.
The molecule to insert can not consist of multiple charge groups.
Make it a single charge group.
Setting the minimum soft core sigma to %g nm
Found environment variable GMX_NB_GENERIC.
Disabling interaction-specific nonbonded kernels.
Found environment variable GMX_NOOPTIMIZEDKERNELS.
Disabling SSE/SSE2/Altivec/ia64/Power6/Bluegene specific kernels.
Table routines are used for coulomb: %s
Table routines are used for vdw:     %s
Will do PME sum in reciprocal space.
Using the Ewald3DC correction for systems with a slab geometry.
Using a Gaussian width (1/beta) of %g nm for Ewald
rvdw_switch (%f) must be < rvdw (%f)
Using %s Lennard-Jones, switch between %g and %g nm
Switch/shift interaction not supported with Buckingham
Cut-off's:   NS: %g   Coulomb: %g   %s: %g
Determining largest Buckingham b parameter for table
Atomtype[%d] = %d, maximum = %d
Buckingham b parameters, min: %g, max: %g
System total charge, top. A: %.3f top. B: %.3f
No fcdata or table file name passed, can not read table, can not do bonded interactions
A charge group has size %d which is larger than the limit of %d atoms
Going to determine what solvent types we have.
Moltype '%s': there are %d atoms in this charge group
Enabling %s-like water optimization for %d molecules.
Found environment variable GMX_NO_SOLV_OPT.
Disabling all solvent optimization
*count
_%s%d.%s
typecount
Counted %d exclusions
fr->f_twin
fr->f_novirsum_alloc
GMX_NO_ALLVSALL
switched
shifted
BHAM
GMX_SCSIGMA_MIN
GMX_NB_GENERIC
GMX_NOOPTIMIZEDKERNELS
GMX_FORCE_TABLES
Essman95a
In-Chul99a
fr->phi
fr->shift_vec
fr->fshift
nbfp
fr->atype_radius
fr->atype_vol
fr->atype_surftens
fr->atype_gb_radius
fr->atype_S_hct
System total charge: %.3f
fr->nblists
fr->gid2nblists
QM/MM calculation requested.
cginfo_mb
bExcl
cginfo_mb[mb].cginfo
cg_sp
cg_sp[mb]
solvent_parameters
GMX_NO_SOLV_OPT
fr->rlist
%s: %e
fr->rcoulomb
fr->fudgeQQ
fr->bGrid
%s: %s
fr->bTwinRange
fr->nblists[i].tab.n
%s: %d
fr->rcoulomb_switch
H@/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/genborn_allvsall_sse2_double.c
aadata->prologue_mask_gb
aadata->epilogue_mask
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/partdec.c
Incorrect entry or number of entries in GMX_CAPACITY='%s'
Internal error in init_partdec: multinr = NULL
CPU=%3d, lastcg=%5d, targetcg=%5d, myshift=%5d
Atomid %d is larger than number of atoms (%d)
pd->shift = %3d, pd->bshift=%3d
Negative number of atoms (%d) on node %d
You have probably not used the same value for -np with grompp and mdrun
Division of bonded forces over processors
Nodeid   atom0   #atom     cg0       #cg
vsitecomm->left_export_construct
vsitecomm->right_export_construct
MPI_Isend Failed
MPI_Wait: result=%d
MPI_Recv Failed
capacity
GMX_CAPACITY
multinr_cgs
multinr_nre
multinr_nre[i]
left_range
right_range
pd->index
pd->cgindex
pd->vbuf
pdc->nlocalatoms
pdc->sendbuf
pdc->recvbuf
%-12s
Workload division
nnodes:       %5d
pd->shift:    %5d
pd->bshift:   %5d
%6d%8d%8d%8d%10d
*list
vsitecomm->send_buf
vsitecomm->recv_buf
state_local
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/tgroup.c
ekind->tcstat
ekind->grpstat
G-Hat
ghat.xvg
%10g  %10g
Error reading from file %s
%d%d%d%d%lf%lf%lf
%lf%lf%lf%lf%lf%lf
gridsize: %10d %10d %10d
spacing:  %10g %10g %10g
ptr1
ptr2
ptr3
Reading ghat of %d %d %d
output.hat
%8d  %8d  %8d  %15.10e  %15.10e %15.10e
%8d  %8d  %8d  %8d  %15.10e  %15.10e  %15.10e
%10g  %10g  %10g  %10g  %10g  %10g
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/ghat.c
Opened %s for reading ghat function
    nalias    porder     niter      bSym      beta[X-Z]
%10d%10d%10d%10d%10g%10g%10g
      acut        r1      pval      zval      eref      qopt
%10g%10g%10g%10g%10g%10g
Successfully read ghat function!
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/groupcoord.c
*anrs_loc
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/coupling.c
Parrinello-Rahman pressure coupling type %s not supported yet
Step %s  Warning: Pressure scaling more than 1%%.
Berendsen pressure coupling type %s not supported yet
Step %s  Warning: pressure scaling more than 1%%, mu: %g %g %g
TC: group %d: T: %g, Lambda: %g
Barostat is coupled to a T-group with no degrees of freedom
P-T-group: %10d Chain %4d ThermV: %15.8f ThermX: %15.8f
TC: group %d: Ekr %g, Ek %g, Ek_new %g, Lambda: %g
Death horror in update_annealing_target_temp (i=%d/%d npoints=%d)
PC: pres
PC: ekin
PC: vir 
PC: box 
PC: pres 
PC: mu   
state->nosehoover_xi
state->nosehoover_vxi
state->therm_integral
state->nhpres_xi
state->nhpres_vxi
scalefac
MassQ->Qinv
trotter_seq
trotter_seq[i]
MassQ->QPinv
0@ charges
dd->cnb
pme_pp
pme_pp->nat
pme_pp->req
pme_pp->stat
 finish
pme_pp->chargeA
pme_pp->chargeB
pme_pp->x
pme_pp->f
cr->dd->pme_recv_f_buf
PP node %d sending to PME node %d: %d%s%s
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/pme_pp.c
PME only node receiving:%s%s%s
Received from PP node %d: %d charges
PME-only node received coordinates before charges
PME-only node received free energy request, but did not receive B-state charges
Received from PP node %d: %d coordinates
PP node %d receiving from PME node %d: virial and energy
MPI_Isend failed in do_pmeonly
PME node sending to PP node %d: virial and energy
Set grid boundaries dim %d: %f %f
/builddir/build/BUILD/gromacs-4.5.1/src/mdlib/nsgrid.c
The coordinates are bounded in %d dimensions
The number of cg's per cell should be > 0
Successfully freed memory for grid pointers.
set_grid_sizes, i-zone bounds for dim %d: %6.3f %6.3f
CG density %f ideal ns cell size %f
grid dim %d size %d x %f: %f - %f
CG ncg ideal %d, actual density %.1f
Number of grid cells is zero. Probably the system and box collapsed.
calc_bor: cg0=%d, cg1=%d, ncg=%d
WARNING: nra_alloc %d cg0 %d cg1 %d cg %d
 ix iy iz   nr  index  cgs...
Explanation: During neighborsearching, we assign each particle to a grid
based on its coordinates. If your system contains collisions or parameter
errors that give particles very high velocities you might end up with some
coordinates being +-Infinity or NaN (not-a-number). Obviously, we cannot
put these on a grid, so this is usually where we detect those errors.
Make sure your system is properly energy-minimized and that the potential
energy seems reasonable before trying again.
GMX_NSCELL_NCG
Set ncg_ideal to %d
grid->cell_index
grid->a
grid->index
grid->nra
grid->dcx2
grid->dcy2
grid->dcz2
Grid: %d x %d x %d cells
CG0[%d]=%d, CG1[%d]=%d
Filling grid from %d to %d
nra=%d, grid->nra=%d, cci=%d
ci = %d, cci = %d
nr:        %d
nrx:       %d
nry:       %d
nrz:       %d
ncg_ideal: %d
    i  cell_index
%3d%3d%3d%5d%5d
libmd_d.so.6.debug
.data
.rodata
.shstrtab
.dynamic
.note.gnu.build-id
.eh_frame
.gnu.hash
.fini
.gnu_debuglink
.dynsym
.gnu.version
.rel.dyn
.data.rel.ro
.gnu.version_r
.jcr
.eh_frame_hdr
.dynstr
.ctors
.dtors
.bss
.init
.rel.plt
.got.plt
.got
.text
