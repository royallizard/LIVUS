{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29648886-8819-4f1b-89dc-6b4496645db2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def train_model():\n",
    "    # Load and prepare training data\n",
    "    file_dir = \"../data/training\"\n",
    "    corpus = []\n",
    "    file_names = []\n",
    "\n",
    "    # iterate over files in training directory\n",
    "    for file in os.listdir(file_dir):\n",
    "        if file.endswith('.txt'):\n",
    "            file_path = os.path.join(file_dir, file)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                contents = f.read()\n",
    "                corpus.append(contents)\n",
    "                file_names.append(file[:-4])\n",
    "\n",
    "    # Create a TfidfVectorizer and transform the corpus\n",
    "    vectorizer = TfidfVectorizer(token_pattern='.+', max_df=0.8)\n",
    "    feature_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Train a RandomForestClassifier\n",
    "    clf = RandomForestClassifier(n_estimators=100)\n",
    "    clf.fit(feature_matrix, file_names)\n",
    "\n",
    "    print(\"Model training complete.\")\n",
    "    return vectorizer, clf\n",
    "\n",
    "def prepare_test_data(vectorizer):\n",
    "    # Load and prepare testing data\n",
    "    test_dir = \"../data/testing\"\n",
    "    test_corpus = []\n",
    "    test_file_names = []\n",
    "\n",
    "    # Iterate over files in testing directory\n",
    "    for file in os.listdir(test_dir):\n",
    "        if file.endswith('.txt'):\n",
    "            file_path = os.path.join(test_dir, file)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                contents = f.read()\n",
    "                test_corpus.append(contents)\n",
    "                test_file_names.append(file[:-4])\n",
    "\n",
    "    # Transform the test corpus using the trained vectorizer\n",
    "    feature_matrix = vectorizer.transform(test_corpus)\n",
    "\n",
    "    print(\"Test data preparation complete.\")\n",
    "    return test_file_names, feature_matrix\n",
    "\n",
    "def highest_predictions(clf, test_file_names, feature_matrix):\n",
    "    # Get the highest predictions for each test file\n",
    "    predictions = clf.predict(feature_matrix)\n",
    "    result = list(zip(test_file_names, predictions))\n",
    "\n",
    "    df = pd.DataFrame(result, columns=['Test File Name', 'Prediction'])\n",
    "    print(\"\\nHighest predictions for each test file:\")\n",
    "    print(df)\n",
    "\n",
    "    return None\n",
    "\n",
    "def threshold_predictions(clf, test_file_names, feature_matrix, threshold):\n",
    "    threshold_predictions_list = []\n",
    "    unique_predictions_set = set()  # store unique predictions\n",
    "\n",
    "    # Get the predictions for each test file\n",
    "    for i in range(len(test_file_names)):\n",
    "        prediction_scores = clf.predict_proba(feature_matrix[i])[0]  # Get the prediction probabilities\n",
    "\n",
    "        # Get predictions above the threshold\n",
    "        above_threshold = clf.classes_[prediction_scores >= threshold]\n",
    "        threshold_predictions_list.append((test_file_names[i], list(above_threshold)))\n",
    "\n",
    "        # Add unique cleaned predictions to set\n",
    "        cleaned_predictions = [pred.split(\".\")[0] for pred in above_threshold]\n",
    "        unique_predictions_set.update(cleaned_predictions)\n",
    "\n",
    "        # Write predictions for each file in a specific directory with a specific name\n",
    "        output_file = os.path.join('../out/exec', f\"{test_file_names[i]}_predictions.txt\")\n",
    "        with open(output_file, 'w') as f:\n",
    "            for prediction in above_threshold:\n",
    "                f.write(prediction + '\\n')\n",
    "\n",
    "    # Printing threshold predictions as separate tables for each test file\n",
    "    for file_name, predictions in threshold_predictions_list:\n",
    "        df = pd.DataFrame(predictions, columns=['Predictions'])\n",
    "        print(f\"\\nPredictions for {file_name}:\")\n",
    "        print(df)\n",
    "\n",
    "    # Writing unique cleaned predictions to a file\n",
    "    with open('../out/exec/predictions.txt', 'w') as f:\n",
    "        for prediction in unique_predictions_set:\n",
    "            f.write(prediction + '\\n')\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_NVD():\n",
    "    print(f\"Now searching NVD for the predicted libraries...\")\n",
    "    stream = os.popen(f'python3 ../util/parse_nvd.py')\n",
    "    output = stream.read()\n",
    "    print(\"Done!\")\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8fe72e5-b425-4ce5-bdfe-4beef099baf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Train your model\n",
    "    vectorizer, clf = train_model()\n",
    "\n",
    "    # Prepare your test data\n",
    "    test_file_names, feature_matrix = prepare_test_data(vectorizer)\n",
    "\n",
    "    # Use this function to get only the highest prediction, e.g. when using it on SOs\n",
    "    # highest_pred_df = highest_predictions(clf, test_file_names, feature_matrix)\n",
    "    \n",
    "    # Use this function to get all identified libraries for given programs\n",
    "    threshold_predictions(clf, test_file_names, feature_matrix, threshold=0.07)\n",
    "    # Run this to parse the NVD. This is currently only possible for the threshold predictions\n",
    "    parse_NVD()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2026f9c-b264-4920-9bf8-ec35a96ca55d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete.\n",
      "Test data preparation complete.\n",
      "\n",
      "Predictions for wireshark:\n",
      "           Predictions\n",
      "0       libcurl.so.4.0\n",
      "1      libexpat.so.1.6\n",
      "2              liblzma\n",
      "3  libpng16.so.16.37.0\n",
      "4       libtiff.so.5.2\n",
      "\n",
      "Predictions for vlc:\n",
      "           Predictions\n",
      "0       libcurl.so.4.0\n",
      "1      libexpat.so.1.6\n",
      "2              liblzma\n",
      "3  libpng16.so.16.37.0\n",
      "4       libtiff.so.5.2\n",
      "\n",
      "Predictions for openttd:\n",
      "            Predictions\n",
      "0        libcurl.so.4.0\n",
      "1       libexpat.so.1.6\n",
      "2  libgnutls.so.30.26.0\n",
      "3               liblzma\n",
      "4   libpng16.so.16.37.0\n",
      "5        libtiff.so.5.2\n",
      "\n",
      "Predictions for gimp-2.10:\n",
      "           Predictions\n",
      "0       libcurl.so.4.0\n",
      "1      libexpat.so.1.6\n",
      "2              liblzma\n",
      "3  libpng16.so.16.37.0\n",
      "4       libtiff.so.5.2\n",
      "Now searching NVD for the predicted libraries...\n",
      "Done!\n",
      "Searching for libexpat...\n",
      "Processing the CVE data...\n",
      "Saved to file\n",
      "Searching for libgnutls...\n",
      "Processing the CVE data...\n",
      "Saved to file\n",
      "Searching for libpng16...\n",
      "Processing the CVE data...\n",
      "Saved to file\n",
      "Searching for liblzma...\n",
      "No vulnerabilities found for liblzma.\n",
      "Searching for libtiff...\n",
      "Processing the CVE data...\n",
      "Saved to file\n",
      "Searching for libcurl...\n",
      "Processing the CVE data...\n",
      "Saved to file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073f0995-cb9b-4732-97b8-1434f4b50209",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
